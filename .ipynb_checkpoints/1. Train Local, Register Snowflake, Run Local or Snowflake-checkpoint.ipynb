{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b51a0ef2-3578-4ad9-ad78-a845986a0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import sproc, col\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "from snowflake.snowpark.types import PandasDataFrameType, IntegerType, StringType, FloatType, Variant\n",
    "from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "from snowflake.ml.fileset import fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb5c97b1-b9e0-4aa4-a306-498d27729d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Warehouse ASYNC_WH successfully created.')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Snowflake Connection Details\n",
    "snowflake_connection_cfg = json.loads(open(\"/Users/mitaylor/Documents/creds/creds.json\").read())\n",
    "\n",
    "# Creating Snowpark Session\n",
    "session = Session.builder.configs(snowflake_connection_cfg).create()\n",
    "\n",
    "# Create a fresh & new schema\n",
    "session.sql(\"CREATE OR REPLACE DATABASE PYTORCH_DEMO\").collect()\n",
    "session.sql('''CREATE OR REPLACE STAGE UDF_STAGE''').collect()\n",
    "session.sql('''CREATE OR REPLACE STAGE FILESET_DEMO\n",
    "  DIRECTORY = ( ENABLE = true )\n",
    "  encryption=(type='SNOWFLAKE_SSE')''').collect()\n",
    "\n",
    "session.sql(\"CREATE OR REPLACE WAREHOUSE ASYNC_WH WITH WAREHOUSE_SIZE='X-SMALL'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1299561d-500d-4338-b3d0-3fb6b9d84dc6",
   "metadata": {},
   "source": [
    "# 1. Create a Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc44765-8116-4ed7-b6a4-8a99f231e849",
   "metadata": {},
   "source": [
    "# 1.1 Load some arbitrary data into Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89cfdf67-7ec0-4be0-afc0-5685e1088068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.table.Table at 0x7fd12bab7dc0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "columns = [str(i) for i in range(0,10)]\n",
    "X,y = make_classification(n_samples=100000, n_features=10, n_classes=2)\n",
    "X = np.array(X, dtype=np.float32)\n",
    "df = pd.DataFrame(X, columns=columns)\n",
    "feature_cols = [\"COL\" + i for i in df.columns]\n",
    "df.columns = feature_cols\n",
    "df['Y'] = y\n",
    "session.write_pandas(df, table_name='DUMMY_DATASET', auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19073c-87b9-470e-a5f5-3e668f2ea628",
   "metadata": {},
   "source": [
    "## 1.2 Create a Fileset Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0fed52b-e291-495a-b7ae-99ba7df1e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = session.table('DUMMY_DATASET')\n",
    "train_sdf, test_sdf = sdf.random_split(weights=[0.8, 0.2], seed=0)\n",
    "train_sdf.write.mode('overwrite').save_as_table('DUMMY_DATASET_TRAIN')\n",
    "test_sdf.write.mode('overwrite').save_as_table('DUMMY_DATASET_TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c84ab99-9ce1-4077-962b-ed5419bfc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "FS_STAGE_NAME = \"FILESET_DEMO\"\n",
    "fileset_train_sdf = fileset.FileSet.make(\n",
    "    target_stage_loc=f\"@{session.get_current_database()}.{session.get_current_schema()}.{FS_STAGE_NAME}/\",\n",
    "    name=\"DUMMY_FILESET_TRAIN\",\n",
    "    snowpark_dataframe=train_sdf,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "fileset_test_sdf = fileset.FileSet.make(\n",
    "    target_stage_loc=f\"@{session.get_current_database()}.{session.get_current_schema()}.{FS_STAGE_NAME}/\",\n",
    "    name=\"DUMMY_FILESET_TEST\",\n",
    "    snowpark_dataframe=test_sdf,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638a113-703d-4c4e-bf01-0f40ccd44b56",
   "metadata": {},
   "source": [
    "# 1.3 Get the Filset locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03a78ce9-db34-4939-b275-ee4c48c8ae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(file='data_01b3fe58-0000-e056-0000-f14900af7e92_016_1_0.snappy.parquet', size=1187428, status='DOWNLOADED', message='')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"GET @FILESET_DEMO/DUMMY_FILESET_TRAIN 'file:///Users/mitaylor/Documents/GitHub/AA Cleaned Repos/simple-pytorch-example/data/train' \").collect()\n",
    "session.sql(\"GET @FILESET_DEMO/DUMMY_FILESET_TEST 'file:///Users/mitaylor/Documents/GitHub/AA Cleaned Repos/simple-pytorch-example/data/test' \").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36c053-05f9-48e8-896b-4d6f17144a72",
   "metadata": {},
   "source": [
    "# 2. Build Neural Net In Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d12ac8d-b19d-4524-a503-dc4f05b4f080",
   "metadata": {},
   "source": [
    "## 2.1 Prep the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31ab3236-f10a-4d1f-b78a-6f928e97f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tens = torch.tensor(X)\n",
    "y_tens = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f65ab472-24e6-4791-98f7-82d18a054ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into PyTorch tensors\n",
    "X_tens = torch.empty_like(X_tens).copy_(X_tens)\n",
    "y_tens = torch.empty_like(y_tens).copy_(y_tens).reshape(-1, 1)\n",
    "loader = DataLoader(list(zip(X_tens,y_tens)), shuffle=True, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1988b676-148b-4d2e-9df4-58228c9b50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor:torch.Tensor):\n",
    "        return self.model(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d68b6-2ecd-44f8-b43d-2617fe67273e",
   "metadata": {},
   "source": [
    "# 3. Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f23ff38f-9d95-4dbc-84a3-f2f56af0ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    n_epochs = 5\n",
    "    device = 'cpu'\n",
    "    model = MyModel()\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training step\n",
    "    for epoch in range(n_epochs):\n",
    "        current_loss = 0.0\n",
    "    \n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "    \n",
    "            X_batch, y_batch = X.to(device), y.to(device)\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "    \n",
    "            # compute loss\n",
    "            loss = loss_fn(y_pred.float(), y_batch.float())\n",
    "    \n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            current_loss += loss.item()\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "           print(f\"Loss after epoch {epoch}: {current_loss}\")\n",
    "           for param in model.parameters():\n",
    "                print(param.data)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Model training complete.')\n",
    "    print(f'Training time: {end_time-start_time}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0069d0ea-0cf7-461f-9442-9c3d708873a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 637.2545770109864\n",
      "tensor([[-1.8317e-02,  1.5106e-01, -1.4328e-02, -1.9379e-01, -6.5999e-03,\n",
      "         -2.1097e-02,  1.3912e-01,  4.0608e-03,  5.8638e-02,  1.1382e-03],\n",
      "        [-2.4535e-01,  1.5937e-01, -5.2283e-02,  2.4372e-01,  7.2789e-02,\n",
      "          1.1512e-01, -1.5283e-01,  1.9260e-01,  7.1760e-02, -9.3361e-02],\n",
      "        [ 3.7031e-02,  7.0855e-02,  5.0988e-02,  5.4159e-02,  1.9298e-02,\n",
      "         -5.1789e-02,  1.3391e-01, -1.4499e-01, -3.7774e-02,  2.0743e-01],\n",
      "        [-1.0262e-02,  2.1683e-01,  7.9308e-03,  2.0104e-01, -2.0156e-01,\n",
      "          1.7413e-04, -1.0076e-02, -7.0204e-03, -6.5704e-03,  1.5473e-02],\n",
      "        [ 5.6176e-03,  5.3171e-01, -1.5312e-02, -3.5281e-02,  4.7479e-01,\n",
      "          4.5552e-01,  8.1175e-03,  2.8066e-02, -2.9128e-02,  2.6518e-02],\n",
      "        [-2.0953e-01, -2.5814e-02, -2.0038e-01, -1.6099e-01,  1.7992e-01,\n",
      "         -1.6004e-01,  1.4637e-01, -1.1547e-01,  1.0821e-01, -1.4588e-01],\n",
      "        [ 1.8236e-01, -5.4621e-02, -1.4506e-01,  1.9343e-01,  1.9273e-01,\n",
      "          1.9171e-01,  1.5536e-01,  6.1767e-02,  6.1516e-03,  1.8715e-01],\n",
      "        [-1.7780e-02,  4.2357e-01, -7.1605e-03,  2.1461e-01, -2.9304e-01,\n",
      "          2.8951e-01, -7.0400e-02,  4.7844e-02, -3.7959e-02,  3.2237e-02],\n",
      "        [ 1.7366e-02,  1.3370e-01,  1.4441e-02,  1.4589e-01, -5.0901e-01,\n",
      "         -1.4480e-01, -2.3680e-02, -1.2301e-02,  8.7132e-03,  2.3582e-02],\n",
      "        [-1.2460e-02, -3.5141e-01,  6.9832e-02, -5.8265e-01,  2.3336e-01,\n",
      "          2.2789e-01,  2.0735e-02, -6.2925e-02,  4.7745e-02,  8.7953e-03]])\n",
      "tensor([ 0.2188, -0.1120, -0.2427, -0.5361, -0.5220, -0.0811,  0.0405, -0.1460,\n",
      "         0.1824,  0.6575])\n",
      "tensor([[ 0.0482, -0.0212, -0.0171, -0.3642, -0.5085, -0.0177, -0.0632,  0.1699,\n",
      "          0.1908, -0.3190]])\n",
      "tensor([0.6918])\n",
      "Model training complete.\n",
      "Training time: 11.401848077774048\n"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deab937-0934-44fc-a407-6cd9a19721dd",
   "metadata": {},
   "source": [
    "# 4. Deploy model (into Registry, then into a UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0773fcf7-6c68-44c0-830c-2b53a0234c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import registry\n",
    "\n",
    "REGISTRY_DATABASE_NAME = \"PYTORCH_DEMO\"\n",
    "REGISTRY_SCHEMA_NAME = \"PUBLIC\"\n",
    "native_registry = registry.Registry(session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62e837ea-f945-4ce8-9db7-4bdb7c8c1101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitaylor/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/contextlib.py:135: UserWarning: `relax_version` is not set and therefore defaulted to True. Dependency version constraints relaxed from ==x.y.z to >=x.y, <(x+1). To use specific dependency versions for compatibility, reproducibility, etc., set `options={'relax_version': False}` when logging the model.\n",
      "  return next(self.gen)\n",
      "/Users/mitaylor/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/ml/model/model_signature.py:69: UserWarning: The sample input has 100000 rows, thus a truncation happened before inferring signature. This might cause inaccurate signature inference. If that happens, consider specifying signature manually.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_ref = native_registry.log_model(\n",
    "    model,\n",
    "    model_name=\"torchModel\",\n",
    "    version_name=\"v1\",\n",
    "    sample_input_data=[X_tens],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd7770f1-8b03-41ec-bdac-89ca57663381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'FORWARD',\n",
       "  'target_method': 'forward',\n",
       "  'signature': ModelSignature(\n",
       "                      inputs=[\n",
       "                          FeatureSpec(dtype=DataType.FLOAT, name='input_feature_0', shape=(10,))\n",
       "                      ],\n",
       "                      outputs=[\n",
       "                          FeatureSpec(dtype=DataType.FLOAT, name='output_feature_0', shape=(1,))\n",
       "                      ]\n",
       "                  )}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ref.show_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "093ce068-b585-4ea1-89a5-a263b17905a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output_feature_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.35751473903656006]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.7751520872116089]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.26887667179107666]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.9140706658363342]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>[0.5455130338668823]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>[0.997458279132843]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>[0.3630709648132324]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>[0.12121051549911499]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            output_feature_0\n",
       "0      [0.35751473903656006]\n",
       "1       [0.7751520872116089]\n",
       "2      [0.26887667179107666]\n",
       "3                        [0]\n",
       "4       [0.9140706658363342]\n",
       "...                      ...\n",
       "99995   [0.5455130338668823]\n",
       "99996    [0.997458279132843]\n",
       "99997                    [0]\n",
       "99998   [0.3630709648132324]\n",
       "99999  [0.12121051549911499]\n",
       "\n",
       "[100000 rows x 1 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ref.run([X_tens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2436448-7787-4bd4-bf5e-ff1c541cbade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"0\"                   |\"1\"                    |\"2\"                 |\"3\"                  |\"4\"                 |\"5\"                  |\"6\"                  |\"7\"                  |\"8\"                  |\"9\"                 |\"y\"  |\"input_feature_0\"          |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|-0.29623791575431824  |-0.042623672634363174  |0.7249094843864441  |-0.4209217131137848  |0.4089137613773346  |0.26310214400291443  |0.40178823471069336  |0.12115471065044403  |-1.9882298707962036  |0.7772805094718933  |1    |[                          |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  -2.962379157543182e-01,  |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  -4.262367263436317e-02,  |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  7.249094843864441e-01,   |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  -4.209217131137848e-01,  |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  4.089137613773346e-01,   |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  2.631021440029144e-01,   |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  4.017882347106934e-01,   |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  1.211547106504440e-01,   |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  -1.988229870796204e+00,  |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  7.772805094718933e-01,   |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |  1                        |\n",
      "|                      |                       |                    |                     |                    |                     |                     |                     |                     |                    |     |]                          |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_data_df = session.sql(\"select DUMMY_DATASET\")\n",
    "input_data_df = input_data_df.drop(\"y\")\n",
    "input_data_df = input_data_df.with_column('\"input_feature_0\"', F.\n",
    "                                          ('*'))\n",
    "input_data_df.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f611a081-d298-418d-8d48-27b1c4105229",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = model_ref.run(input_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0f74316c-e7c2-42d7-99e6-5617efab5456",
   "metadata": {},
   "outputs": [
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 01b3fe5a-0000-e048-0000-f14900af8612: 100357 (P0000): Python Interpreter Error:\nTraceback (most recent call last):\n  File \"/home/udf/4048102905/forward.py\", line 78, in infer\n    predictions_df = runner(input_df[input_cols])\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/snowflake/ml/model/_packager/model_handlers/pytorch.py\", line 186, in fn\n    res = getattr(raw_model, target_method)(*t)\n  File \"/var/folders/97/8vc6xcbx4zd06p75xg9frdrw0000gn/T/ipykernel_28105/452384569.py\", line 12, in forward\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 116, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (10x11 and 10x10)\n in function FORWARD with handler forward.infer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictions_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/telemetry.py:139\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mquery_history() \u001b[38;5;28;01mas\u001b[39;00m query_history:\n\u001b[0;32m--> 139\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     plan \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_select_statement \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_plan\n\u001b[1;32m    141\u001b[0m     api_calls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;241m*\u001b[39mplan\u001b[38;5;241m.\u001b[39mapi_calls,\n\u001b[1;32m    143\u001b[0m         {TelemetryField\u001b[38;5;241m.\u001b[39mNAME\u001b[38;5;241m.\u001b[39mvalue: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    144\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/dataframe.py:3050\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, max_width, statement_params)\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[38;5;129m@df_collect_api_telemetry\u001b[39m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3037\u001b[0m     statement_params: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3038\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3039\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates this DataFrame and prints out the first ``n`` rows with the\u001b[39;00m\n\u001b[1;32m   3040\u001b[0m \u001b[38;5;124;03m    specified maximum number of characters per column.\u001b[39;00m\n\u001b[1;32m   3041\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;124;03m        statement_params: Dictionary of statement level parameters to be set while executing this action.\u001b[39;00m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3049\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m-> 3050\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_or_update_statement_params_with_query_tag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_statement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m                \u001b[49m\u001b[43mSKIP_LEVELS_TWO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3059\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/dataframe.py:3168\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, max_width, **kwargs)\u001b[0m\n\u001b[1;32m   3165\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mqueries[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m   3167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sql_select_statement(query):\n\u001b[0;32m-> 3168\u001b[0m     result, meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_and_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3172\u001b[0m     res, meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mget_result_and_metadata(\n\u001b[1;32m   3173\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   3174\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:652\u001b[0m, in \u001b[0;36mServerConnection.get_result_and_metadata\u001b[0;34m(self, plan, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_result_and_metadata\u001b[39m(\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28mself\u001b[39m, plan: SnowflakePlan, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    651\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Row], List[Attribute]]:\n\u001b[0;32m--> 652\u001b[0m     result_set, result_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     result \u001b[38;5;241m=\u001b[39m result_set_to_rows(result_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    654\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m convert_result_meta_to_attribute(result_meta)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:190\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     ne \u001b[38;5;241m=\u001b[39m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[1;32m    188\u001b[0m         e\n\u001b[1;32m    189\u001b[0m     )\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ne\u001b[38;5;241m.\u001b[39mwith_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:121\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    123\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:612\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[0;34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m holder, id_ \u001b[38;5;129;01min\u001b[39;00m placeholders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    611\u001b[0m     final_query \u001b[38;5;241m=\u001b[39m final_query\u001b[38;5;241m.\u001b[39mreplace(holder, id_)\n\u001b[0;32m--> 612\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_job_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m placeholders[query\u001b[38;5;241m.\u001b[39mquery_id_place_holder] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    626\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mquery_id\n\u001b[1;32m    627\u001b[0m )\n\u001b[1;32m    628\u001b[0m result_meta \u001b[38;5;241m=\u001b[39m get_new_description(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:123\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m    120\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:117\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m    120\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[1;32m    121\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:417\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m         query_id_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to execute query\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:402\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_statement_params\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNOWPARK_SKIP_TXN_COMMIT_IN_DDL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m--> 402\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_and_notify_query_listener\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecute query [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_cursor\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:354\u001b[0m, in \u001b[0;36mServerConnection.execute_and_notify_query_listener\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_and_notify_query_listener\u001b[39m(\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SnowflakeCursor:\n\u001b[0;32m--> 354\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify_query_listeners(\n\u001b[1;32m    356\u001b[0m         QueryRecord(results_cursor\u001b[38;5;241m.\u001b[39msfqid, results_cursor\u001b[38;5;241m.\u001b[39mquery)\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results_cursor\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/connector/cursor.py:1136\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1133\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1134\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[0;32m-> 1136\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/connector/errors.py:290\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    269\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    298\u001b[0m             error_class,\n\u001b[1;32m    299\u001b[0m             error_value,\n\u001b[1;32m    300\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/connector/errors.py:345\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 345\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/snowflake/connector/errors.py:221\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    219\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    222\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    223\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    224\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    225\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    226\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    227\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    229\u001b[0m     ),\n\u001b[1;32m    230\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    231\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    232\u001b[0m )\n",
      "\u001b[0;31mSnowparkSQLException\u001b[0m: (1304): 01b3fe5a-0000-e048-0000-f14900af8612: 100357 (P0000): Python Interpreter Error:\nTraceback (most recent call last):\n  File \"/home/udf/4048102905/forward.py\", line 78, in infer\n    predictions_df = runner(input_df[input_cols])\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/snowflake/ml/model/_packager/model_handlers/pytorch.py\", line 186, in fn\n    res = getattr(raw_model, target_method)(*t)\n  File \"/var/folders/97/8vc6xcbx4zd06p75xg9frdrw0000gn/T/ipykernel_28105/452384569.py\", line 12, in forward\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/lib/python_udf/c0ec591262b5905d2537dfcb5a546dbd94104d87acd8f3fb45088ac394f55e81/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 116, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (10x11 and 10x10)\n in function FORWARD with handler forward.infer"
     ]
    }
   ],
   "source": [
    "predictions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85277b-4758-40e6-a124-bd38321a12ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fe5cf8f-d885-4407-8602-144adb72ab0a",
   "metadata": {},
   "source": [
    "# 5. Run it on a Fileset in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7dccc84-2fa9-4928-89d3-c1521393af37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ModelVersion in module snowflake.ml.model._client.model.model_version_impl object:\n",
      "\n",
      "class ModelVersion(builtins.object)\n",
      " |  ModelVersion() -> None\n",
      " |  \n",
      " |  Model Version Object representing a specific version of the model that could be run.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, _ModelVersion__value: object) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  delete_metric(self, metric_name: str) -> None\n",
      " |      Delete a metric from metric storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          metric_name: The name of the metric to be deleted.\n",
      " |      \n",
      " |      Raises:\n",
      " |          KeyError: When the requested metric name does not exist.\n",
      " |  \n",
      " |  get_metric(self, metric_name: str) -> Any\n",
      " |      Get the value of a specific metric.\n",
      " |      \n",
      " |      Args:\n",
      " |          metric_name: The name of the metric.\n",
      " |      \n",
      " |      Raises:\n",
      " |          KeyError: When the requested metric name does not exist.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The value of the metric.\n",
      " |  \n",
      " |  run(self, X: Union[pandas.core.frame.DataFrame, snowflake.snowpark.dataframe.DataFrame], *, function_name: Optional[str] = None, strict_input_validation: bool = False) -> Union[pandas.core.frame.DataFrame, snowflake.snowpark.dataframe.DataFrame]\n",
      " |      Invoke a method in a model version object.\n",
      " |      \n",
      " |      Args:\n",
      " |          X: The input data, which could be a pandas DataFrame or Snowpark DataFrame.\n",
      " |          function_name: The function name to run. It is the name used to call a function in SQL.\n",
      " |              Defaults to None. It can only be None if there is only 1 method.\n",
      " |          strict_input_validation: Enable stricter validation for the input data. This will result value range based\n",
      " |              type validation to make sure your input data won't overflow when providing to the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: When no method with the corresponding name is available.\n",
      " |          ValueError: When there are more than 1 target methods available in the model but no function name specified.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The prediction data. It would be the same type dataframe as your input.\n",
      " |  \n",
      " |  set_metric(self, metric_name: str, value: Any) -> None\n",
      " |      Set the value of a specific metric.\n",
      " |      \n",
      " |      Args:\n",
      " |          metric_name: The name of the metric.\n",
      " |          value: The value of the metric.\n",
      " |  \n",
      " |  show_functions(self) -> List[snowflake.ml.model._model_composer.model_manifest.model_manifest_schema.ModelFunctionInfo]\n",
      " |      Show all functions information in a model version that is callable.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of ModelFunctionInfo objects containing the following information:\n",
      " |      \n",
      " |          - name: The name of the function to be called (both in SQL and in Python SDK).\n",
      " |          - target_method: The original method name in the logged Python object.\n",
      " |          - signature: Python signature of the original method.\n",
      " |  \n",
      " |  show_metrics(self) -> Dict[str, Any]\n",
      " |      Show all metrics logged with the model version.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary showing the metrics.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  fully_qualified_model_name\n",
      " |      Return the fully qualified name of the model to which the model version belongs.\n",
      " |  \n",
      " |  model_name\n",
      " |      Return the name of the model to which the model version belongs, usable as a reference in SQL.\n",
      " |  \n",
      " |  version_name\n",
      " |      Return the name of the version to which the model version belongs, usable as a reference in SQL.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  comment\n",
      " |      The comment to the model version.\n",
      " |  \n",
      " |  description\n",
      " |      The description for the model version. This is an alias of `comment`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_model_name': <class 'snowflake.ml._internal.utils...\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d3427-4de9-48a3-8f25-0ae36ee9a5e9",
   "metadata": {},
   "source": [
    "# Do the next cell in a sproc or UDF for server side inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "680313a2-6c91-410d-b606-bbafc27fce20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark:SFFileSystem.info() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFFileSystem._open() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFStageFileSystem._open() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': tensor([-0.5607, -0.2020, -0.1885,  0.5232]), '1': tensor([0.3300, 0.7217, 1.0068, 0.5628]), '2': tensor([-0.2890,  0.3478, -0.2592, -0.5748]), '3': tensor([0.4056, 0.8343, 0.5201, 0.6205]), '4': tensor([ 0.9908, -1.1942,  0.5714, -1.9183]), '5': tensor([-0.8134, -0.4245, -0.0218,  0.5493]), '6': tensor([-0.9227, -0.3730, -2.2369,  1.1993]), '7': tensor([-0.5810,  0.3710,  0.2800, -0.8917]), '8': tensor([ 1.7019,  0.6239,  0.5516, -1.5708]), '9': tensor([-1.1930, -0.7236,  0.1613,  0.6446]), 'y': tensor([1, 1, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitaylor/opt/anaconda3/envs/snowpark_pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:62: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_f904_uudri/croot/pytorch-select_1707783714699/work/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  return torch.as_tensor(data)\n"
     ]
    }
   ],
   "source": [
    "# Use FileSet to get data from a Snowflake table in the form of files in an internal server-side excrypted stage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#write to a fileset\n",
    "\n",
    "STAGE_NAME = \"FILESET_DEMO\"\n",
    "fileset_test_df = fileset.FileSet(\n",
    "    target_stage_loc=f\"@{session.get_current_database()}.{session.get_current_schema()}.{STAGE_NAME}/\",\n",
    "    name=\"DUMMY_FILESET_TEST\",\n",
    "    snowpark_session=session,\n",
    ")\n",
    "\n",
    "pipe = fileset_test_df.to_torch_datapipe(\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    drop_last_batch=True)\n",
    "\n",
    "for batch in DataLoader(pipe, batch_size=None, num_workers=0):\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba3fa71e-9a76-4aca-b8e3-9d48dd0b4bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "snowflake.ml.fileset.torch_datapipe.ReadAndParseParquet"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74634522-4e44-4d69-8845-0c935f4bf03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e823f4-29d7-4078-a6ad-490c3f02a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = native_registry.get_model(MODEL_NAME).version(MODEL_VERSION)\n",
    "model_.run(sdf_filt_test, function_name=\"predict\").write.save_as_table(\"ML_PREDICT\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "@udf(...)\n",
    "def predict_with_pytorch()\n",
    "    model_ = native_registry.get_model(MODEL_NAME).version(MODEL_VERSION)\n",
    "    turns it into tensors\n",
    "    results = model_ref.run([X_tens])\n",
    "    returns results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10076c0c-ad99-4e9e-b80b-9dc7acdbe589",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sdf.with_column('PREDICTION', pytorch_udf_model(*feature_cols)).to_pandas()\n",
    "#where the internals convert it into a fileset/tensor and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96c69c80-2bbc-41a6-960d-e6116b7fa5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or sproc this\n",
    "\n",
    "#Use FileSet to get data from a Snowflake table in the form of files in an internal server-side excrypted stage\n",
    "fileset_train_df = fileset.FileSet(\n",
    "    target_stage_loc=f\"@{session.get_current_database()}.{session.get_current_schema()}.{STAGE_NAME}/\",\n",
    "    name=\"diamonds_train\",\n",
    "    snowpark_session=session,\n",
    ")\n",
    "\n",
    "# Feed Training FileSet to Pytorch\n",
    "# Get PyTorch DataPipe\n",
    "train_dp = fileset_train_df.to_torch_datapipe(batch_size=32*4, shuffle=True, drop_last_batch=False)\n",
    "# Shard the training DataPipe so each GPU gets a different subset of data\n",
    "train_dp = train_dp.sharding_filter()\n",
    "\n",
    "# Pass PyTorch DataPipe to Pytorch DataLoader\n",
    "dataloader = DataLoader(train_dp, batch_size=None, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af95631-c035-44f2-92f7-edf8f42e0c61",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Do the wrapper function\n",
    "\n",
    "Do it in a SProc\n",
    "\n",
    "Do it in SPCS\n",
    "\n",
    "Do it in SPCS with DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a647a0-4b51-4e88-964b-d689fd9d8168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd196d97-81b9-402b-beca-aefee003ce48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c9d3014-6275-4940-8eb2-0e6ffe3ecac4",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63dde8-f506-4463-a427-f677f852751d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d8562-55ab-4b05-9534-ed43b54139ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee872bc8-77e9-406c-b3aa-9ef1955c3ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3eec99-f68a-4099-adf8-91aa5683e7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754a6b6b-4a2f-4bcd-bab1-34c012ad0ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Training + Testing Datasets\n",
    "#with FileLock(\"/tmp/data.lock\"):\n",
    "training_data = datasets.FashionMNIST(root=\"/tmp/data\", train=True, download=True, transform=ToTensor(),)\n",
    "test_data = datasets.FashionMNIST(root=\"/tmp/data\", train=False, download=True, transform=ToTensor(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c02820d-079e-4087-971b-0829782c1419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7f91cb-4236-4eb0-86f8-c6846402a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition & Training\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.two_hidden_layer_nn = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        digits = self.two_hidden_layer_nn(x)\n",
    "        return digits\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"loss: {loss.item()} [{(batch+1)* len(X)}/{size}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "rank = int(os.environ['LOCAL_RANK']) # Used to identify the local node\n",
    "world_size = int(os.environ['WORLD_SIZE']) # Total number of CPUs/GPUs available\n",
    "dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "model = MyModel()\n",
    "batch_size = 64\n",
    "device = 'cpu'\n",
    "model = DDP(model)\n",
    "\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "epochs = 3\n",
    "for ep in range(epochs):\n",
    "    print(f\"Epoch {ep}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea8a45-4127-4118-84b7-00fd7261b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(26, 1), nn.ReLU())\n",
    "\n",
    "    def forward(self, tensor:torch.Tensor):\n",
    "        return self.model(tensor)\n",
    "\n",
    "\n",
    "def get_batch(batch):\n",
    "    X_batch = torch.column_stack(\n",
    "        (\n",
    "            batch[\"CUT_OE_IDEAL\"],\n",
    "            batch[\"CUT_OE_PREMIUM\"],\n",
    "            batch[\"CUT_OE_VERY_GOOD\"],\n",
    "            batch[\"CUT_OE_GOOD\"],\n",
    "            batch[\"CUT_OE_FAIR\"],\n",
    "            batch[\"COLOR_OE_D\"],\n",
    "            batch[\"COLOR_OE_E\"],\n",
    "            batch[\"COLOR_OE_F\"],\n",
    "            batch[\"COLOR_OE_G\"],\n",
    "            batch[\"COLOR_OE_H\"],\n",
    "            batch[\"COLOR_OE_I\"],\n",
    "            batch[\"COLOR_OE_J\"],\n",
    "            batch[\"CLARITY_OE_IF\"],\n",
    "            batch[\"CLARITY_OE_VVS1\"],\n",
    "            batch[\"CLARITY_OE_VVS2\"],\n",
    "            batch[\"CLARITY_OE_VS1\"],\n",
    "            batch[\"CLARITY_OE_VS2\"],\n",
    "            batch[\"CLARITY_OE_SI1\"],\n",
    "            batch[\"CLARITY_OE_SI2\"],\n",
    "            batch[\"CLARITY_OE_I1\"],\n",
    "            batch[\"CLARITY_OE_I2\"],\n",
    "            batch[\"CLARITY_OE_I3\"],\n",
    "            batch[\"CARAT\"],\n",
    "            batch[\"X\"],\n",
    "            batch[\"Y\"],\n",
    "            batch[\"Z\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return X_batch\n",
    "\n",
    "STAGE_NAME = 'FILESET_DEMO'\n",
    "device = 'cpu'\n",
    "\n",
    "print(f\"rank = {rank}, using device {device}\")\n",
    "\n",
    "# Use FileSet to get data from a Snowflake table in the form of files in an internal server-side excrypted stage\n",
    "fileset_train_df = fileset.FileSet(\n",
    "    target_stage_loc=f\"@{session.get_current_database()}.{session.get_current_schema()}.{STAGE_NAME}/\",\n",
    "    name=\"diamonds_train\",\n",
    "    snowpark_session=session,\n",
    ")\n",
    "\n",
    "# Feed Training FileSet to Pytorch\n",
    "# Get PyTorch DataPipe\n",
    "train_dp = fileset_train_df.to_torch_datapipe(batch_size=32*4, shuffle=True, drop_last_batch=False)\n",
    "# Shard the training DataPipe so each GPU gets a different subset of data\n",
    "train_dp = train_dp.sharding_filter()\n",
    "\n",
    "# Pass PyTorch DataPipe to Pytorch DataLoader\n",
    "dataloader = DataLoader(train_dp, batch_size=None, num_workers=0)\n",
    "\n",
    "# Define model & training params\n",
    "model = MyModel()\n",
    "model = model.to(device)\n",
    "\n",
    "n_epochs = 100\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "start_time = time.time()\n",
    "# Training step\n",
    "for epoch in range(n_epochs):\n",
    "    current_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        X_batch = get_batch(batch)\n",
    "        y_batch = torch.column_stack((batch[\"PRICE\"],))\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "       print(f\"[RANK = {rank}] Loss after epoch {epoch}: {current_loss / 32*4}\")\n",
    "       for param in model.parameters():\n",
    "            print(param.data)\n",
    "\n",
    "end_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
