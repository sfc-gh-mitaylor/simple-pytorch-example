{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51a0ef2-3578-4ad9-ad78-a845986a0595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import sproc, col\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "from snowflake.snowpark.types import PandasDataFrameType, IntegerType, StringType, FloatType, Variant\n",
    "from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "from snowflake.ml.fileset import fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5c97b1-b9e0-4aa4-a306-498d27729d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Warehouse ASYNC_WH successfully created.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Snowflake Connection Details\n",
    "snowflake_connection_cfg = json.loads(open(\"/Users/mitaylor/Documents/creds/creds.json\").read())\n",
    "\n",
    "# Creating Snowpark Session\n",
    "session = Session.builder.configs(snowflake_connection_cfg).create()\n",
    "\n",
    "# Create a fresh & new schema\n",
    "session.sql(\"CREATE OR REPLACE DATABASE PYTORCH_DEMO\").collect()\n",
    "session.sql('''CREATE OR REPLACE STAGE UDF_STAGE''').collect()\n",
    "session.sql('''CREATE OR REPLACE STAGE FILESET_DEMO\n",
    "  DIRECTORY = ( ENABLE = true )\n",
    "  encryption=(type='SNOWFLAKE_SSE')''').collect()\n",
    "\n",
    "session.sql(\"CREATE OR REPLACE WAREHOUSE ASYNC_WH WITH WAREHOUSE_SIZE='X-SMALL'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1299561d-500d-4338-b3d0-3fb6b9d84dc6",
   "metadata": {},
   "source": [
    "# 1. Create a Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc44765-8116-4ed7-b6a4-8a99f231e849",
   "metadata": {},
   "source": [
    "# 1.1 Load some arbitrary data into Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cfdf67-7ec0-4be0-afc0-5685e1088068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.table.Table at 0x7fc260f2f6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "columns = [str(i) for i in range(0,10)]\n",
    "X,y = make_classification(n_samples=100000, n_features=10, n_classes=2)\n",
    "X = np.array(X, dtype=np.float32)\n",
    "df = pd.DataFrame(X, columns=columns)\n",
    "feature_cols = [\"COL\" + i for i in df.columns]\n",
    "df.columns = feature_cols\n",
    "df['Y'] = y\n",
    "session.write_pandas(df, table_name='DUMMY_DATASET', auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19073c-87b9-470e-a5f5-3e668f2ea628",
   "metadata": {},
   "source": [
    "## 1.2 Create a Fileset Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0fed52b-e291-495a-b7ae-99ba7df1e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = session.table('DUMMY_DATASET')\n",
    "train_sdf, test_sdf = sdf.random_split(weights=[0.8, 0.2], seed=0)\n",
    "train_sdf.write.mode('overwrite').save_as_table('DUMMY_DATASET_TRAIN')\n",
    "test_sdf.write.mode('overwrite').save_as_table('DUMMY_DATASET_TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c84ab99-9ce1-4077-962b-ed5419bfc699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FileSet.files() is in private preview since 0.2.0. Do not use it in production. \n",
      "SFFileSystem.ls() is in private preview since 0.2.0. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:SFStageFileSystem.ls() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    }
   ],
   "source": [
    "FS_STAGE_NAME = \"FILESET_DEMO\"\n",
    "fileset_train_sdf = fileset.FileSet.make(\n",
    "    target_stage_loc=f\"@{session.get_current_database()}.{session.get_current_schema()}.{FS_STAGE_NAME}/\",\n",
    "    name=\"DUMMY_FILESET_TRAIN\",\n",
    "    snowpark_dataframe=train_sdf,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "fileset_test_sdf = fileset.FileSet.make(\n",
    "    target_stage_loc=f\"@{session.get_current_database()}.{session.get_current_schema()}.{FS_STAGE_NAME}/\",\n",
    "    name=\"DUMMY_FILESET_TEST\",\n",
    "    snowpark_dataframe=test_sdf,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638a113-703d-4c4e-bf01-0f40ccd44b56",
   "metadata": {},
   "source": [
    "# 1.3 Get the Filset locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03a78ce9-db34-4939-b275-ee4c48c8ae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(file='data_01b403f2-0000-e099-0000-f14900b1365e_016_1_0.snappy.parquet', size=1187446, status='DOWNLOADED', message='')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"GET @FILESET_DEMO/DUMMY_FILESET_TRAIN 'file:///Users/mitaylor/Documents/GitHub/AA Cleaned Repos/simple-pytorch-example/data/train' \").collect()\n",
    "session.sql(\"GET @FILESET_DEMO/DUMMY_FILESET_TEST 'file:///Users/mitaylor/Documents/GitHub/AA Cleaned Repos/simple-pytorch-example/data/test' \").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36c053-05f9-48e8-896b-4d6f17144a72",
   "metadata": {},
   "source": [
    "# 2. Build Neural Net In Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d12ac8d-b19d-4524-a503-dc4f05b4f080",
   "metadata": {},
   "source": [
    "## 2.1 Prep the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cee8ccb7-29a1-464b-8c9d-b99cd716d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark.session:Package 'pytorch' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"training complete, model loss: 283.8899168477219\"'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sproc_training(session: Session) -> Variant:\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    from torch import nn\n",
    "    import torch.optim as optim\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from torch.utils.data import DataLoader\n",
    "    import os\n",
    "    import torch.distributed as dist\n",
    "    from snowflake.ml.fileset import fileset\n",
    "\n",
    "    def get_batch(batch):\n",
    "        X_batch = torch.column_stack(\n",
    "            (\n",
    "                batch[\"COL0\"],\n",
    "                batch[\"COL1\"],\n",
    "                batch[\"COL2\"],\n",
    "                batch[\"COL3\"],\n",
    "                batch[\"COL4\"],\n",
    "                batch[\"COL5\"],\n",
    "                batch[\"COL6\"],\n",
    "                batch[\"COL7\"],\n",
    "                batch[\"COL8\"],\n",
    "                batch[\"COL9\"],\n",
    "            )\n",
    "        )\n",
    "        return X_batch\n",
    "    \n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MyModel, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(10, 10),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(10, 1),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "    \n",
    "        def forward(self, tensor:torch.Tensor):\n",
    "            return self.model(tensor)\n",
    "   \n",
    "    def train_model(loader):\n",
    "        n_epochs = 5\n",
    "        device = 'cpu'\n",
    "\n",
    "        # Define model & training params\n",
    "        model = MyModel()\n",
    "\n",
    "        #########\n",
    "        # # Distributed Data Parallel wrapper which will take care of model weights averaging and syncing\n",
    "        # # This works for the case where the model weights can fit in a single CPU/GPU but the data is too large and can be split\n",
    "        # os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        # os.environ['MASTER_PORT'] = '12355'\n",
    "        # dist.init_process_group(\"gloo\", rank=1, world_size=4) # Use NCCL backend for distributed GPU training\n",
    "        # model = model.to(device)\n",
    "        # model = DDP(model, device_ids=[device], output_device=device)\n",
    "        #########\n",
    "        \n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training step\n",
    "        for epoch in range(n_epochs):\n",
    "            current_loss = 0.0\n",
    "            for i, batch in enumerate(loader):\n",
    "\n",
    "                X_batch = get_batch(batch)\n",
    "                y_batch = torch.column_stack((batch[\"Y\"],))\n",
    "\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "        \n",
    "                # compute loss\n",
    "                loss = loss_fn(y_pred.float(), y_batch.float())\n",
    "        \n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                current_loss += loss.item()\n",
    "        \n",
    "            if epoch % 10 == 0:\n",
    "               print(f\"Loss after epoch {epoch}: {current_loss}\")\n",
    "               for param in model.parameters():\n",
    "                    print(param.data)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print('Model training complete.')\n",
    "        print(f'Training time: {end_time-start_time}')\n",
    "        return model, X_batch, current_loss   \n",
    "\n",
    "\n",
    "    # Use FileSet to get data from a Snowflake table in the form of files in an internal server-side excrypted stage\n",
    "    fileset_train_df = fileset.FileSet(\n",
    "        target_stage_loc=\"@PYTORCH_DEMO.PUBLIC.FILESET_DEMO/\",\n",
    "        name=\"DUMMY_FILESET_TRAIN\",\n",
    "        snowpark_session=session,\n",
    "    )\n",
    "\n",
    "    pipe = fileset_train_df.to_torch_datapipe(\n",
    "       batch_size=16,\n",
    "       shuffle=True,\n",
    "       drop_last_batch=True)\n",
    "    loader = DataLoader(pipe, batch_size=None, num_workers=0)\n",
    "\n",
    "    model, X_batch, current_loss = train_model(loader)\n",
    "\n",
    "    # Register the Model\n",
    "    from snowflake.ml.registry import registry\n",
    "    REGISTRY_DATABASE_NAME = \"PYTORCH_DEMO\"\n",
    "    REGISTRY_SCHEMA_NAME = \"PUBLIC\"\n",
    "    native_registry = registry.Registry(\n",
    "        session=session,\n",
    "        database_name=REGISTRY_DATABASE_NAME,\n",
    "        schema_name=REGISTRY_SCHEMA_NAME)\n",
    "    model_ref = native_registry.log_model(\n",
    "        model,\n",
    "        model_name=\"torchModelSProc\",\n",
    "        version_name=\"v3\",\n",
    "        sample_input_data=[X_batch],)\n",
    "\n",
    "    result = f\"training complete, model loss: {current_loss}\"\n",
    "    return result\n",
    "\n",
    "# Register sproc\n",
    "sproc_training = session.sproc.register(\n",
    "    func=sproc_training, \n",
    "    name='YOUR_SPROC_NAME', \n",
    "    is_permanent=True, \n",
    "    replace=True,\n",
    "    stage_location='@UDF_STAGE', \n",
    "    packages=['snowflake-snowpark-python', 'pytorch', 'snowflake-ml-python', 'cryptography', 'torchdata'])\n",
    "\n",
    "sproc_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
